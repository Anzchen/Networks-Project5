#!/usr/bin/env python3

import argparse
import re
import socket
import ssl
from html.parser import HTMLParser

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443


class LinkParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.links = []

    def handle_starttag(self, tag, attrs):
        if tag == 'a':
            for attr in attrs:
                if attr[0] == 'href':
                    self.links.append(attr[1])


class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
        self.notVisited = []
        self.visited = set()
        self.secret_flags = set()

    def create_socket(self):
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        mysocket.connect((self.server, self.port))
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        mysocket = ssl_context.wrap_socket(mysocket, server_hostname=self.server)
        return mysocket

    # send all request in series to the server
    def send_pipeline_requests(self, sock, requests):
        for request in requests:
            sock.sendall(request.encode('ascii'))

    # function that make receive the data as a pipeline
    def receive_pipeline_responses(self, sock, num_responses):
        responses = []
        buffer = b""
        while num_responses > 0:
            while True:
                data = sock.recv(4096)
                if not data:
                    break
                buffer += data

            # find the break point of the response
            while b'\r\n\r\n' in buffer:
                # split at this point and return the rest to the buffer
                response, buffer = buffer.split(b'\r\n\r\n', 1)

                # if there is html end tag, meaning this part is continued with the previous header
                if b'</html>' in buffer:
                    response_continue, buffer = buffer.split(b'</html>', 1)
                    response += response_continue
                responses.append(response.decode('ascii'))

                # minus 1 for checking out whether receive all response, end at 0
                num_responses -= 1
        return responses

    def run(self):
        request = f"GET /accounts/login/ HTTP/1.1\r\nHost: {self.server}\r\nConnection: close\r\n\r\n"
        mysocket = self.create_socket()
        self.send_pipeline_requests(mysocket, [request])
        data = self.receive_pipeline_responses(mysocket, 1)[0]
        mysocket.close()

        # Find and get the CSRF token
        csrfPos = data.find('csrfmiddlewaretoken')
        csrfVal = data.find('value', csrfPos + 1)
        csrf = data[csrfVal + 7: csrfVal + 39]

        request_body = f"username={self.username}&password={self.password}&csrfmiddlewaretoken={csrf}&next=%2Ffakebook%2F"
        content_length = len(request_body)

        login_request = f"POST /accounts/login/ HTTP/1.1\r\nHost: {self.server}\r\nConnection: close\r\nContent-Length: {content_length}\r\nContent-Type: application/x-www-form-urlencoded\r\nCookie: csrftoken={csrf};\r\n\r\n{request_body} "

        mysocket = self.create_socket()
        self.send_pipeline_requests(mysocket, [login_request])
        data = self.receive_pipeline_responses(mysocket, 1)[0]
        mysocket.close()

        # Find and get session ID
        SessionPos = data.find("sessionid=")
        sessionID = data[SessionPos + 10:SessionPos + 42]

        # refresh and get the CSRF token
        csrfPos = data.find('csrftoken=')
        csrf = data[csrfPos + 10: csrfPos + 74]

        self.notVisited.append(f"https://{self.server}/fakebook/")

        # loop for visiting new pages
        while len(self.secret_flags) < 5 and len(self.notVisited) > 0:
            requests = []

            # pull out the resquest from the notVisited list
            while len(requests) < 10 and len(self.notVisited) > 0:
                page = self.notVisited.pop()
                # already visited then jump over this one
                if page in self.visited:
                    continue
                self.visited.add(page)
                # check if it is last request of the series of request, in order to close the server
                if len(requests) == 9 or len(self.notVisited) == 0:
                    # it is the last one of the queue, then make a close server request
                    nextPage = f"GET {page} HTTP/1.1\r\nHost: {self.server}\r\nConnection: close\r\nCookie: csrftoken={csrf};sessionid={sessionID};\r\n\r\n"
                else:
                    # it is not the last one of the queue, then make a keep-alive server request
                    nextPage = f"GET {page} HTTP/1.1\r\nHost: {self.server}\r\nConnection: keep-alive\r\nCookie: csrftoken={csrf};sessionid={sessionID};\r\n\r\n"
                requests.append(nextPage)

            mysocket = self.create_socket()
            self.send_pipeline_requests(mysocket, requests)
            responses = self.receive_pipeline_responses(mysocket, len(requests))
            mysocket.close()

            for res in responses:
                # refresh and get session ID
                SessionPos = res.find("sessionid=")
                if SessionPos != -1:
                    sessionID = res[SessionPos + 10:SessionPos + 42]

                # refresh and get the CSRF token
                csrfPos = res.find('csrftoken=')
                if csrfPos != -1:
                    csrf = data[csrfPos + 10: csrfPos + 74]

                status = re.findall(r"\D(\d{3})\D", res)

                if status[0] == '200':
                    # Handle 200 - OK
                    flags = re.findall(r"FLAG: (\w{64})", res)
                    for flag in flags:
                        self.secret_flags.add(flag)
                    parser = LinkParser()
                    parser.feed(res)
                    new_links = [f"https://{self.server}{link}" for link in parser.links if
                                 f"https://{self.server}{link}" not in self.visited
                                 and f"https://{self.server}{link}" not in self.notVisited]
                    for link in new_links:
                        self.notVisited.append(link)

                elif status[0] == '302':
                    # Handle 302 - Found (Redirect)
                    location = re.search(r"Location: (.*)\r\n", res)
                    if location:
                        new_url = location.group(1)
                        if new_url.startswith("/"):
                            new_url = f"https://{self.server}{new_url}"
                        if new_url.startswith(f"https://{self.server}/fakebook/"):
                            self.notVisited.append(new_url)

                elif status[0] in ('403', '404'):
                    # Handle 403 - Forbidden and 404 - Not Found
                    # Do nothing, just abandon the URL
                    pass

                elif status[0] == '503':
                    # Handle 503 - Service Unavailable
                    # it is not going to happen
                    pass

        # After the while loop, print the found secret flags
        for flag in self.secret_flags:
            print(flag)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
